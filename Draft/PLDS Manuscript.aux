\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Model}{2}}
\citation{roweis1999unifying}
\newlabel{eq:model}{{3}{3}{The Model}{}{}}
\citation{roweis1999unifying}
\newlabel{eq:model0}{{4}{4}{The Model}{}{}}
\citation{shumway1982approach}
\citation{ghahramani1996parameter}
\citation{van1994n4sid}
\citation{doretto2003dynamic}
\citation{bootslearning}
\newlabel{eqn:penaltylik}{{5}{5}{The Model}{}{}}
\newlabel{eqn:penaltylikdual}{{6}{5}{The Model}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Parameter Estimation}{5}}
\newlabel{eqn:loglik}{{7}{6}{Parameter Estimation}{}{}}
\newlabel{eqn:penaltylik2}{{8}{6}{Parameter Estimation}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}E Step}{6}}
\newlabel{eq:expecs}{{9}{6}{E Step}{}{}}
\citation{turlach2005simultaneous}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}M Step}{7}}
\newlabel{eq:updateR}{{10}{7}{M Step}{}{}}
\newlabel{eq:penaltylik1}{{11}{7}{M Step}{}{}}
\citation{tikhonov1943stability}
\citation{beck2009fast}
\citation{daubechies2004iterative}
\newlabel{eq:vectorizec}{{12}{8}{M Step}{}{}}
\newlabel{eq:penaltylik11}{{13}{8}{M Step}{}{}}
\newlabel{eq:updatec}{{14}{8}{M Step}{}{}}
\newlabel{eq:penaltylik2}{{15}{8}{M Step}{}{}}
\newlabel{eq:penaltylik21}{{16}{8}{M Step}{}{}}
\newlabel{eq:updatea}{{17}{9}{M Step}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Initialization}{9}}
\newlabel{sec:initial}{{3.3}{9}{Initialization}{}{}}
\newlabel{eq:initial}{{18}{9}{Initialization}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The Complete EM}{9}}
\newlabel{sec:em}{{3.4}{9}{The Complete EM}{}{}}
\citation{landman2011multi}
\citation{van2013wu}
\citation{moeller2010multiband}
\citation{feinberg2010multiplexed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Improving Computational Efficiency}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}The Data}{10}}
\citation{kuhn1955hungarian}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parameter Estimation}{11}}
\newlabel{sec:lowdsim}{{4.1}{11}{Parameter Estimation}{}{}}
\newlabel{eq:distance}{{19}{11}{Parameter Estimation}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases.\relax }}{12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:low-high-d-sim}{{1}{12}{x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases.\relax }{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Low dimensional setting}}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {High dimensional setting}}}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }}{13}}
\newlabel{fig:heatmap}{{2}{13}{Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\sc  \texttt  {Mr}.\nobreakspace  {}\texttt  {Sid}}\nobreakspace  {}Running Time\relax }}{13}}
\newlabel{tab:runningTime}{{1}{13}{\mrsid ~Running Time\relax }{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Making Predictions}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimation and prediction accuracies. The x-axis is the penalty size under log scale, while y-axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }}{14}}
\newlabel{fig:estpredaccuracy}{{3}{14}{Estimation and prediction accuracies. The x-axis is the penalty size under log scale, while y-axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application}{14}}
\newlabel{sec:application}{{5}{14}{Application}{}{}}
\citation{smith2004advances}
\citation{meier2008complex}
\citation{zhu2006automatic}
\citation{amari1996new}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }}{16}}
\newlabel{fig:eigvals}{{4}{16}{Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Similarities Among Estimated $A$ Matrices\relax }}{17}}
\newlabel{tab:similarity}{{2}{17}{Similarities Among Estimated $A$ Matrices\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }}{17}}
\newlabel{fig:cgraph}{{5}{17}{Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }{}{}}
\citation{nebel2014disruption}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }}{18}}
\newlabel{fig:matsim}{{6}{18}{Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }}{18}}
\newlabel{fig:3d}{{7}{18}{3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }{}{}}
\citation{allen2014generalized}
\citation{arbabshirani2014impact}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Prediction accuracies comparison on HCP data. \emph  {Left:} The mean squared error (MSE) is used as the accuracy measure. \emph  {Right:} Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by {\sc  \texttt  {Mr}.\nobreakspace  {}\texttt  {Sid}}\nobreakspace  {}model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose. \relax }}{19}}
\newlabel{fig:predaccy}{{8}{19}{Prediction accuracies comparison on HCP data. \emph {Left:} The mean squared error (MSE) is used as the accuracy measure. \emph {Right:} Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by \mrsid ~model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose. \relax }{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{19}}
\newlabel{sec:appendix1}{{6}{20}{Appendix 1}{}{}}
\newlabel{sec:appendix2}{{6}{21}{Appendix 2}{}{}}
\newlabel{eqn: fistatarget}{{20}{21}{Appendix 2}{}{}}
\newlabel{sec:appendix3}{{6}{22}{Appendix 3}{}{}}
\newlabel{sec:appendix4}{{6}{22}{Appendix 4}{}{}}
\bibstyle{plain}
\bibdata{reference}
\bibcite{allen2014generalized}{1}
\bibcite{amari1996new}{2}
\bibcite{arbabshirani2014impact}{3}
\bibcite{beck2009fast}{4}
\bibcite{bootslearning}{5}
\bibcite{daubechies2004iterative}{6}
\bibcite{doretto2003dynamic}{7}
\bibcite{feinberg2010multiplexed}{8}
\bibcite{ghahramani1996parameter}{9}
\bibcite{kuhn1955hungarian}{10}
\bibcite{landman2011multi}{11}
\bibcite{meier2008complex}{12}
\bibcite{moeller2010multiband}{13}
\bibcite{nebel2014disruption}{14}
\bibcite{roweis1999unifying}{15}
\bibcite{shumway1982approach}{16}
\bibcite{smith2004advances}{17}
\bibcite{tikhonov1943stability}{18}
\bibcite{turlach2005simultaneous}{19}
\bibcite{van2013wu}{20}
\bibcite{van1994n4sid}{21}
\bibcite{zhu2006automatic}{22}
