\relax 
\citation{Johansen1988}
\citation{Friston2003a}
\citation{Xie2013a}
\citation{Kalman1963}
\citation{Kalman1960a}
\citation{Ljung1998}
\citation{Hsieh2013}
\citation{Banerjee2013a}
\citation{CHEN1989}
\citation{Zaharia2010}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\citation{rabiner1989tutorial}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Model}{5}}
\newlabel{eq:model}{{3}{6}{The Model}{}{}}
\citation{roweis1999unifying}
\citation{roweis1999unifying}
\newlabel{eq:model0}{{4}{9}{The Model}{}{}}
\newlabel{eqn:penaltylik}{{5}{9}{The Model}{}{}}
\newlabel{eqn:penaltylikdual}{{6}{9}{The Model}{}{}}
\citation{shumway1982approach}
\citation{ghahramani1996parameter}
\citation{van1994n4sid}
\citation{doretto2003dynamic}
\citation{bootslearning}
\@writefile{toc}{\contentsline {section}{\numberline {3}Parameter Estimation}{10}}
\newlabel{eqn:loglik}{{7}{11}{Parameter Estimation}{}{}}
\newlabel{eqn:penaltylik2}{{8}{11}{Parameter Estimation}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}E Step}{12}}
\newlabel{eq:expecs}{{9}{12}{E Step}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}M Step}{12}}
\citation{turlach2005simultaneous}
\newlabel{eq:updateR}{{10}{13}{M Step}{}{}}
\newlabel{eq:penaltylik1}{{11}{13}{M Step}{}{}}
\citation{tikhonov1943stability}
\newlabel{eq:vectorizec}{{12}{14}{M Step}{}{}}
\newlabel{eq:penaltylik11}{{13}{14}{M Step}{}{}}
\newlabel{eq:updatec}{{14}{14}{M Step}{}{}}
\newlabel{eq:penaltylik2}{{15}{14}{M Step}{}{}}
\citation{beck2009fast}
\citation{daubechies2004iterative}
\newlabel{eq:penaltylik21}{{16}{15}{M Step}{}{}}
\newlabel{eq:updatea}{{17}{15}{M Step}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Initialization}{16}}
\newlabel{sec:initial}{{3.3}{16}{Initialization}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The Complete EM}{16}}
\newlabel{sec:em}{{3.4}{16}{The Complete EM}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Improving Computational Efficiency}{16}}
\citation{landman2011multi}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The Complete EM Algorithm\relax }}{17}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:em}{{1}{17}{The Complete EM Algorithm\relax }{}{}}
\citation{van2013wu}
\citation{moeller2010multiband}
\citation{feinberg2010multiplexed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}The Data}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parameter Estimation}{18}}
\newlabel{sec:lowdsim}{{4.1}{18}{Parameter Estimation}{}{}}
\citation{kuhn1955hungarian}
\newlabel{eq:distance}{{18}{19}{Parameter Estimation}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases.\relax }}{21}}
\newlabel{fig:low-high-d-sim}{{1}{21}{x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases.\relax }{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Low dimensional setting}}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {High dimensional setting}}}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }}{22}}
\newlabel{fig:heatmap}{{2}{22}{Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\sc  \texttt  {Mr}.\nobreakspace  {}\texttt  {Sid}}\nobreakspace  {}Running Time\relax }}{23}}
\newlabel{tab:runningTime}{{2}{23}{\mrsid ~Running Time\relax }{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Making Predictions}{23}}
\citation{smith2004advances}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimation and prediction accuracies. The x-axis is the penalty size under log scale, while y-axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }}{24}}
\newlabel{fig:estpredaccuracy}{{3}{24}{Estimation and prediction accuracies. The x-axis is the penalty size under log scale, while y-axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application}{24}}
\newlabel{sec:application}{{5}{24}{Application}{}{}}
\citation{meier2008complex}
\citation{zhu2006automatic}
\citation{amari1996new}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Similarities Among Estimated $A$ Matrices\relax }}{27}}
\newlabel{tab:similarity}{{3}{27}{Similarities Among Estimated $A$ Matrices\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }}{27}}
\newlabel{fig:eigvals}{{4}{27}{Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }}{28}}
\newlabel{fig:cgraph}{{5}{28}{Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }{}{}}
\citation{nebel2014disruption}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }}{29}}
\newlabel{fig:matsim}{{6}{29}{Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }}{30}}
\newlabel{fig:3d}{{7}{30}{3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Prediction accuracies comparison on HCP data. \emph  {Left:} The mean squared error (MSE) is used as the accuracy measure. \emph  {Right:} Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by {\sc  \texttt  {Mr}.\nobreakspace  {}\texttt  {Sid}}\nobreakspace  {}model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose. \relax }}{31}}
\newlabel{fig:predaccy}{{8}{31}{Prediction accuracies comparison on HCP data. \emph {Left:} The mean squared error (MSE) is used as the accuracy measure. \emph {Right:} Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by \mrsid ~model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose. \relax }{}{}}
\citation{allen2014generalized}
\citation{arbabshirani2014impact}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{32}}
\newlabel{sec:appendix1}{{6}{33}{Appendix 1}{}{}}
\newlabel{sec:appendix2}{{6}{34}{Appendix 2}{}{}}
\newlabel{eqn: fistatarget}{{19}{34}{Appendix 2}{}{}}
\newlabel{sec:appendix3}{{6}{36}{Appendix 3}{}{}}
\newlabel{sec:appendix4}{{6}{37}{Appendix 4}{}{}}
\bibstyle{ieeetr}
\bibdata{reference}
\bibcite{Johansen1988}{1}
\bibcite{Friston2003a}{2}
\bibcite{Xie2013a}{3}
\bibcite{Kalman1963}{4}
\bibcite{Kalman1960a}{5}
\bibcite{Ljung1998}{6}
\bibcite{Hsieh2013}{7}
\bibcite{Banerjee2013a}{8}
\bibcite{CHEN1989}{9}
\bibcite{Zaharia2010}{10}
\bibcite{rabiner1989tutorial}{11}
\bibcite{roweis1999unifying}{12}
\bibcite{shumway1982approach}{13}
\bibcite{ghahramani1996parameter}{14}
\bibcite{van1994n4sid}{15}
\bibcite{doretto2003dynamic}{16}
\bibcite{bootslearning}{17}
\bibcite{turlach2005simultaneous}{18}
\bibcite{tikhonov1943stability}{19}
\bibcite{beck2009fast}{20}
\bibcite{daubechies2004iterative}{21}
\bibcite{landman2011multi}{22}
\bibcite{van2013wu}{23}
\bibcite{moeller2010multiband}{24}
\bibcite{feinberg2010multiplexed}{25}
\bibcite{kuhn1955hungarian}{26}
\bibcite{smith2004advances}{27}
\bibcite{meier2008complex}{28}
\bibcite{zhu2006automatic}{29}
\bibcite{amari1996new}{30}
\bibcite{nebel2014disruption}{31}
\bibcite{allen2014generalized}{32}
\bibcite{arbabshirani2014impact}{33}
