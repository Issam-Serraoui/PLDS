\relax 
\citation{lindquist2008statistical}
\citation{andersen1999principal}
\citation{calhoun2009review}
\citation{mckeown1998spatially}
\citation{roweis1999unifying}
\citation{rabiner1986introduction}
\citation{rauch1963solutions}
\citation{roweis1999unifying}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{lauritzen1988local}
\citation{pearl1988probabilistic}
\citation{eavani2013unsupervised}
\citation{valdes2005estimating}
\citation{havlicek2011dynamic}
\citation{wang2014systematic}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Model}{3}}
\newlabel{eq:model}{{1}{3}{The Model}{}{}}
\citation{roweis1999unifying}
\citation{roweis1999unifying}
\newlabel{eq:model0}{{2}{5}{The Model}{}{}}
\newlabel{eqn:penaltylik}{{3}{5}{The Model}{}{}}
\newlabel{eqn:penaltylikdual}{{4}{5}{The Model}{}{}}
\citation{shumway1982approach}
\citation{ghahramani1996parameter}
\citation{van1994n4sid}
\citation{doretto2003dynamic}
\citation{bootslearning}
\@writefile{toc}{\contentsline {section}{\numberline {3}Parameter Estimation}{6}}
\newlabel{eqn:loglik}{{5}{7}{Parameter Estimation}{}{}}
\newlabel{eqn:penaltylik2}{{6}{7}{Parameter Estimation}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}E Step}{7}}
\newlabel{eq:expecs}{{7}{7}{E Step}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}M Step}{7}}
\citation{turlach2005simultaneous}
\newlabel{eq:updateR}{{8}{8}{M Step}{}{}}
\newlabel{eq:penaltylik1}{{9}{8}{M Step}{}{}}
\citation{tikhonov1943stability}
\citation{beck2009fast}
\citation{daubechies2004iterative}
\newlabel{eq:vectorizec}{{10}{9}{M Step}{}{}}
\newlabel{eq:penaltylik11}{{11}{9}{M Step}{}{}}
\newlabel{eq:updatec}{{12}{9}{M Step}{}{}}
\newlabel{eq:penaltylik2}{{13}{9}{M Step}{}{}}
\newlabel{eq:penaltylik21}{{14}{9}{M Step}{}{}}
\newlabel{eq:updatea}{{15}{10}{M Step}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Initialization}{10}}
\newlabel{sec:initial}{{3.3}{10}{Initialization}{}{}}
\newlabel{eq:initial}{{16}{10}{Initialization}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The Complete EM}{10}}
\newlabel{sec:em}{{3.4}{10}{The Complete EM}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Improve Computational Efficiency}{11}}
\citation{landman2011multi}
\citation{van2013wu}
\citation{moeller2010multiband}
\citation{feinberg2010multiplexed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}The Data}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Result}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parameter Estimation}{12}}
\newlabel{sec:lowdsim}{{4.1}{12}{Parameter Estimation}{}{}}
\citation{kuhn1955hungarian}
\newlabel{eq:distance}{{17}{13}{Parameter Estimation}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases..\relax }}{14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:low-high-d-sim}{{1}{14}{x axis is tuning parameter $\lambda _C$ under log scale and y axis is the distance between truth and estimations; $\lambda _A$ is increasing proportionally with $\lambda _C$. One can see that in both the low dimensional and hight dimensional setting, estimation accuracies for $A$ and $C$ first increase then decrease as penalty increases..\relax }{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Low dimensional setting}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {High dimensional setting}}}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }}{14}}
\newlabel{fig:heatmap}{{2}{15}{Row 1: A truth; non-penalized estimation of A; optimally penalized estimation of A. Row 2: C truth; non-penalized estimation of C; optimally penalized estimation of C.\relax }{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces PLDS Running Time\relax }}{15}}
\newlabel{tab:runningTime}{{1}{15}{PLDS Running Time\relax }{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Making Predictions}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimation and prediction accuracies.x axis is the penalty size under log scale, while y axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }}{16}}
\newlabel{fig:estpredaccuracy}{{3}{16}{Estimation and prediction accuracies.x axis is the penalty size under log scale, while y axis is the estimation and prediction accuracies. One can see that the penalty that yields the most accurate estimation also gives the best predictions.\relax }{}{}}
\citation{smith2004advances}
\citation{meier2008complex}
\citation{zhu2006automatic}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application}{17}}
\newlabel{sec:application}{{5}{17}{Application}{}{}}
\citation{amari1996new}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }}{18}}
\newlabel{fig:eigvals}{{4}{18}{Eigen-values and corresponding profile likelihood plot from the first scan of subject one. For this data, the profile likelihood picks $d=11$ as the number of latent states.\relax }{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Similarities Among Estimated $A$ Matrices\relax }}{19}}
\newlabel{tab:similarity}{{2}{19}{Similarities Among Estimated $A$ Matrices\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }}{19}}
\newlabel{fig:cgraph}{{5}{19}{Connectivity Graph: The wider edge means stronger connectivity; the red edge means negative connectivity and blue edge means positive connectivity.\relax }{}{}}
\citation{nebel2014disruption}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }}{20}}
\newlabel{fig:matsim}{{6}{20}{Similarities among the four estimated $A$ matrices. The distance $d(\cdot ,\cdot )$ is used in this figure. As one can see, the two red/orange off-diagonal pixels has the minimum distances, which correspond to the pairs of $(A_{11},A_{12})$ and $(A_{21},A_{22})$ respectively. With this similarity map, one can tell which two scans are from the same subject.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }}{20}}
\newlabel{fig:3d}{{7}{21}{3D rendering of columns of matrix $C$: estimation from the first scan of subject one shown in this plot.\relax }{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Prediction accuracies comparison on HCP data. The mean squared error (MSE) is used as the accuracy measure.\relax }}{21}}
\newlabel{fig:predaccy}{{8}{21}{Prediction accuracies comparison on HCP data. The mean squared error (MSE) is used as the accuracy measure.\relax }{}{}}
\citation{arbabshirani2014impact}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by PLDS model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose.\relax }}{22}}
\newlabel{fig:samplets}{{9}{22}{Sample time series plot. The dotted green curve stands for the $60\%$ confidence band given by PLDS model. The true time series is averaged signals from a subsample of voxels. The predictions are also averaged over the same subsample. The confidence band is estimated based on the covariance matrix of these voxels. A subsample of 20 voxels are picked in this experiment to avoid big covariance matrices calculation. All values are log-scaled for plotting purpose.\relax }{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{22}}
\newlabel{sec:appendix1}{{6}{23}{Appendix 1}{}{}}
\newlabel{sec:appendix2}{{6}{24}{Appendix 2}{}{}}
\newlabel{eqn: fistatarget}{{18}{24}{Appendix 2}{}{}}
\newlabel{sec:appendix3}{{6}{25}{Appendix 3}{}{}}
\newlabel{sec:appendix4}{{6}{25}{Appendix 4}{}{}}
\bibstyle{plain}
\bibdata{reference}
\bibcite{amari1996new}{1}
\bibcite{andersen1999principal}{2}
\bibcite{arbabshirani2014impact}{3}
\bibcite{beck2009fast}{4}
\bibcite{bootslearning}{5}
\bibcite{calhoun2009review}{6}
\bibcite{daubechies2004iterative}{7}
\bibcite{doretto2003dynamic}{8}
\bibcite{eavani2013unsupervised}{9}
\bibcite{feinberg2010multiplexed}{10}
\bibcite{ghahramani1996parameter}{11}
\bibcite{havlicek2011dynamic}{12}
\bibcite{kuhn1955hungarian}{13}
\bibcite{landman2011multi}{14}
\bibcite{lauritzen1988local}{15}
\bibcite{lindquist2008statistical}{16}
\bibcite{mckeown1998spatially}{17}
\bibcite{meier2008complex}{18}
\bibcite{moeller2010multiband}{19}
\bibcite{nebel2014disruption}{20}
\bibcite{pearl1988probabilistic}{21}
\bibcite{rabiner1986introduction}{22}
\bibcite{rauch1963solutions}{23}
\bibcite{roweis1999unifying}{24}
\bibcite{shumway1982approach}{25}
\bibcite{smith2004advances}{26}
\bibcite{tikhonov1943stability}{27}
\bibcite{turlach2005simultaneous}{28}
\bibcite{valdes2005estimating}{29}
\bibcite{van2013wu}{30}
\bibcite{van1994n4sid}{31}
\bibcite{wang2014systematic}{32}
\bibcite{zhu2006automatic}{33}
