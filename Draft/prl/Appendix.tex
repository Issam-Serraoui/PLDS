%&latex
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amsthm}
%\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\providecommand{\mb}[1]{\boldsymbol{#1}}
\newcommand{\bx}{\mb{x}}
\newcommand{\by}{\mb{y}}
\newcommand{\bX}{\mb{X}}
\newcommand{\bY}{\mb{Y}}

\let\oldref\ref
\renewcommand{\ref}[1]{(\oldref{#1})}

%\newcommand{\T}{^{\ensuremath{\mathsf{T}}}} % transpose
\newcommand{\T}{^{\ensuremath{\mathsf{T}}}}           % transpose
\newcommand{\mrsid}{{\sc \texttt{Mr}.~\texttt{Sid}}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\diag}{\operatornamewithlimits{diag}}



\begin{document}

\title{Appendix}

\section*{Appendix 1: Standard Kalman Filter Smoother}
\label{sec:appendix1}

\begin{tabular}{l}
\hline
\textbf{Algorithm } Standard Kalman Filter Smoother for estimating the moments \\
$\qquad\quad\quad$ required in the E-step of an EM algorithm for a linear dynamical system\\
\hline
0. Define $\bx_t^{\tau}$ = E($\bx_t|\bY_1^{\tau}$),$\mathbf{V}_t^{\tau}=\text{Var}(\bx_t|\bY_1^{\tau})$, $\hat{\bx}_t \equiv \bx_t^T$ and $\hat{P}_t\equiv V_t^T+\bx_t^T{\bx_t^T}^{\T}$\\
1. Forward Recursions:\\
\hspace{4 mm} $\bx_t^{t-1}=A\bx_{t-1}^{t-1}$\\
\hspace{4 mm} $\mathbf{V}_t^{t-1}=A\mathbf{V}_{t-1}^{t-1}+\mathbf{Q}$\\
\hspace{4 mm} $K_t=\mathbf{V}_t^{t-1}C^{\T}(CV_t^{t-1}C^{\T}+R)^{-1}$\\
\hspace{4 mm} $\bx_t^t$ = $\bx_t^{t-1} + K_t (\by_t - C\bx_t^{t-1})$\\
\hspace{4 mm} $V_t^t=V_t^{t-1}-K_tCV_t^{t-1}$\\
\hspace{4 mm} $\bx_1^0=\mathbf{\pi}_0$, $V_1^0=\mathbf{V}_0$\\
2. Backward Recursions:\\
\hspace{4 mm} $J_{t-1} = V_{t-1}^{t-1}A^{\T}(V_t^{t-1})^{-1}$\\
\hspace{4 mm} $\bx_{t-1}^T=\bx_{t-1}^{t-1}+J_{t-1}(\mathbf{x_t^T-A\bx_{t-1}^{t-1}})$\\
\hspace{4 mm} $V_{t-1}^T = V_{t-1}^{t-1}+J_{t-1}(V_t^T-V_t^{t-1})J_{t-1}^{\T}$\\
\hspace{4 mm} $\hat{P}_{t,t-1}\equiv V_{t,t-1}^T+\bx_t^T{\bx_t^T}^{\T}$\\
\hspace{4 mm} $V_{T,T-1}^T=(I-K_TC)AV_{T-1}^{T-1}$\\
\hline
\end{tabular}

\section*{Appendix 2: Derivation of The EM Algorithm}
By the chain rule, the full likelihood is
\begin{equation*}\label{eqn:likelihood}
\begin{aligned}
P(\bX,\bY)&=P(\bY|\bX) P(\bX)= P(\bx_0)\prod\limits_{t=1}^{T}P(\bx_t|\bx_{t-1})\prod\limits_{t=1}^{T} P(\by_t|\bx_t)\\
&=\prod\limits_{t=1}^{T}P(\bx_t|\bx_{t-1})\prod\limits_{t=1}^{T} P(\by_t|\bx_t)\mathbbm{1}_{\mathbf{\pi}_0}(\bx_0)
\end{aligned}
\end{equation*}
where $\mathbbm{1}_{\mathbf{\pi}_0}(\bx_0)$ is the indicator function. Conditional likelihoods are
\begin{equation*}\label{eqn:condlik}
\begin{aligned}
P(\by_t|\bx_t)&= (2\pi)^{-\frac{p}{2}}|R|^{-\frac{1}{2}}\  \text{exp}\left\{-\frac{1}{2}[\by_t-C\bx_t]^{\T}R^{-1}[\by_t-C\bx_t]\right\}\\
P(\bx_t|\bx_{t-1})
%%&=\text{exp}\left\{-\frac{1}{2}[\mathbf{x_t}-A\mathbf{x_{t-1}}]^{\T}Q^{-1}[\mathbf{x_t}-A\mathbf{x_{t-1}}]\right\}(2\pi)^{-d/2}|Q|^{-1/2}\\
&=(2\pi)^{-\frac{d}{2}}\  \text{exp}\left\{-\frac{1}{2}[\bx_t-A\bx_{t-1}]^{\T}[\bx_t-A\bx_{t-1}]\right\}
\end{aligned}
\end{equation*}

Then the log-likelihood, after dropping a constant, is just a sum of quadratic terms:
\begin{equation*}\label{eqn:loglik}
\begin{split}
\log  P(\bX,\bY)=&-\sum\limits_{t=1}^{T}\big(\frac{1}{2}[\by_t-C\bx_t]^{\T}R^{-1}[\by_t-C\bx_t]\big)-\frac{T}{2}\text{log}|R|\\
&-\sum\limits_{t=1}^{T}\big(\frac{1}{2}[\bx_t-A\bx_{t-1}]^{\T}[\bx_t-A\bx_{t-1}]\big)-\frac{T}{2}\text{log}|\mathbf{I}|\\
&+ \text{log}(\mathbbm{1}_{\mathbf{\pi}_0}(\bx_0)).
\end{split}
\end{equation*}

Then the optimization problem boils down to
\begin{equation}\label{eqn:penaltylik2}
\begin{split}
\hat{\theta}=\argmin_{\substack{\theta}}\biggl\{&\sum\limits_{t=1}^{T}\big(\frac{1}{2}[\by_t-C\bx_t]^{\T}R^{-1}[\by_t-C\bx_t]\big)-\frac{T}{2}\text{log}|R|\\
&+\sum\limits_{t=1}^{T}\big(\frac{1}{2}[\bx_t-A\bx_{t-1}]^{\T}[\bx_t-A\bx_{t-1}]\big)-\frac{T}{2}\text{log}|\mathbf{I}|\\
&- \text{log}(\mathbbm{1}_{\mathbf{\pi}_0}(\bx_0))+\lambda_1\|A\|_1+\lambda_2\|C\|_2^2\biggr\}\\
\end{split}
\end{equation}

Let the target function in the curly braces be denoted  as $\mathbf{\Phi}(\theta,\bY,\bX)$. Then $\mathbf{\Phi}$ can be optimized with \mrsid, a generalized Expectation-Maximization (EM) algorithm.

\subsection*{E Step}
The E step of EM requires computation of the expected log likelihood, $\Gamma = E[\log P(\bX,\bY)|\bY]$. This quantity depends on three expectations: $E[\bx_t|\bY]$, $E[\bx_t\bx_t^{\T}|\bY]$ and $E[\bx_t\bx_{t-1}^{\T}|\bY]$. For simplicity, we denote their finite sample estimators by:
\begin{equation}\label{eq:expecs}
%\begin{aligned}
\hat{\bx}_t \equiv E[\mathbf{x_t}|\bY],\  \hat{P}_t  \equiv E[\bx_t\bx_t^{\T}|\bY],\  \hat{P}_{t,t-1}  \equiv E[\bx_t\bx_{t-1}^{\T}|\bY].
%\end{aligned}
\end{equation}

Expectations \ref{eq:expecs} are estimated with a Kalman filter/smoother (KFS), which is detailed in the Appendix. Notice that all expectations are taken with respect to the current estimations of parameters.
\subsection*{M Step}
Each of the parameters in $\theta =\{A,C,R,\mathbf{\pi}_0\}$ is estimated by taking the corresponding partial derivatives of $\mathbf{\Phi}(\theta,\bY,\bx)$, setting them to zero, and then solving the equations.

Let the estimations from the previous step be denoted as $\theta^{\text{old}} =\{A^{\text{old}},C^{\text{old}},R^{\text{old}},\mathbf{\pi}_0^{\text{old}}\}$ and the current estimations as $\theta^{\text{new}} =\{A^{\text{new}},C^{\text{new}},R^{\text{new}},\mathbf{\pi}_0^{\text{new}}\}$. The estimation for the $R$ matrix has a closed form, as follows:
\begin{equation*}\label{eq:updateR}
\begin{aligned}
\frac{\partial \mathbf{\Phi}}{\partial R^{-1}} &= \frac{T}{2}R - \sum\limits_{t=1}^T\bigl(\frac{1}{2}\by_t\by_t^{\T} - C\hat{\bx}_t\by_t^{\T}+\frac{1}{2}C\hat{P}_tC^{\T}\bigr) =0 \\
\implies R &= \frac{1}{T}\sum\limits_{t=1}^{T}(\by_t\by_t^{\T}-C\hat{\bx}_t\by_t^{\T})\\
\implies R^{\text{new}} &= \diag \biggl\{\frac{1}{T}\sum\limits_{t=1}^{T}(\by_t\by_t^{\T}-C^{\text{new}}\hat{\bx}_t\by_t^{\T})\biggr\}
\end{aligned}
\end{equation*}
In the bottom line, $\diag$ extracts only the diagonal of the in-bracket term, as we constrain $R$ to be diagonal in Constraint 4.

The estimation for $\mathbf{\pi}_0$ has a closed form. The relevant term $\log(\mathbbm{1}_{\mathbf{\pi}_0}(\hat{\bx}_0))$ is minimized only when $\mathbf{\pi}_0^{\text{new}} = \hat{\bx}_0$.

The estimation for the $C$ matrix also has a closed form.
Terms relevant to $C$ are
\begin{equation}\label{eq:penaltylik1}
f_{\lambda_2}(C;\bX,\bY) = \sum\limits_{t=1}^{T}\left(\frac{1}{2}[\by_t-C\bx_t]^{\T}R^{-1}[\by_t-C\bx_t]\right)+\lambda_2 \|C\|_2.
\end{equation}

In $f_{\lambda_2}(C;\bX,\bY)$, $C$ is a matrix, we vectorized it to ease optimization and notation. Without loss of generality, assume $R$ is the identity matrix in equation \ref{eq:penaltylik1}; otherwise, one can always write equation \ref{eq:penaltylik1} as
\begin{equation*}
\sum\limits_{t=1}^{T}\left(\frac{1}{2}[R^{-\frac{1}{2}}y_t-R^{-\frac{1}{2}}Cx_t]^{\T}[R^{-\frac{1}{2}y_t}-R^{-\frac{1}{2}}Cx_t]\right) + \lambda_2 \|R^{-\frac{1}{2}}C\|
\end{equation*}
Let $\bY' = (y_{11},\ldots,y_{T1},y_{12},\ldots,y_{T2},\ldots,y_{1p},\ldots,y_{Tp})^{\T}$
%\begin{equation*}
%\bY' = (y_{11},\ldots,y_{T1},y_{12},\ldots,y_{T2},\ldots,y_{1p},\ldots,y_{Tp})^{\T}
%\end{equation*}
be a $Tp\times 1$ vector from rearranging  $\bY$. In addition, let
\[
\bX' = \begin{pmatrix}
\bX^{\T}&&\\
&\ddots&\\
&&\bX^{\T}
\end{pmatrix}_{pT\times pd}.
\]
% where $W=\biggl(\bx_1,\ldots,\bx_{T}\biggr)$.
Finally, vectorize $C^{\text{old}}$ as
\begin{equation}\label{eq:vectorizec}
\mathbf{c}^{\text{old}} = (C_{11}^{\text{old}},\ldots,C_{1d}^{\text{old}},C_{21}^{\text{old}},\ldots,C_{2d}^{\text{old}},C_{p1}^{\text{old}},\ldots,C_{pd}^{\text{old}})^{\T}
\end{equation}
where $C_{ij}$ is the element at row $i$ and column $j$ of $C$. With these new notations, the equation \ref{eq:penaltylik1} is equivalent to
\begin{equation}\label{eq:penaltylik11}
f_{\lambda_2}(C;\bX,\bY) = \|\bY'  - \mathbf{X' c}\|_2^2 + \lambda_2\|\mathbf{c}\|_2^2.
\end{equation}
With the Tikhonov regularization, equation \ref{eq:penaltylik11} has closed form solution
\begin{equation*}\label{eq:updatec}
\begin{aligned}
\mathbf{c}^{\text{new}} &= (\bX'^{\T}\bX' + \lambda_2\mathbf{I})^{-1}\bX'^{\T}\bY'\\
C^{\text{new}} &=\text{Rearrange } \mathbf{c}^{\text{new}} \text{ by equation }\ref{eq:vectorizec}
\end{aligned}
\end{equation*}


In $f_{\lambda_2}(C;\bX,\bY)$, $C$ is a matrix. To simplify notation and optimization, we vectorized it to a vector $\mathbf{c}$ following the methods of Turlach et al. (2005). A closed form solution for $\mathbf{c}$, denoted $\mathbf{c}^{\text{new}}$, is given by the Tikhonov regularization. By rearranging the elements in $\mathbf{c}^{\text{new}}$, one gets an estimation of matrix $C$. That is,
\begin{equation*}\label{eq:updatec}
C^{\text{new}} =\text{Rearrange } \mathbf{c}^{\text{new}}
\end{equation*}


Now consider matrix $A$. Terms involving $A$ in Eq.~\ref{eqn:penaltylik2} are
\begin{equation*}
f_{\lambda_1}(A;\bX,\bY) = \sum\limits_{t=1}^{T}\big(\frac{1}{2}[\bx_t-A\bx_{t-1}]^{\T}[\bx_t-A\bx_{t-1}]\big)+\lambda_1 \|A\|_1
\end{equation*}

Similar to what we have done to $C$, $f_{\lambda_1}(A;\bX,\bY)$ is equivalent to
\begin{equation*}
f_{\lambda_1}(A;\bX,\bY) =  \|\mathbf{z}  - \mathbf{Za}\|_2^2 + \lambda_1\|\mathbf{a}\|_1
\end{equation*}
where $\mathbf{z}$ is a $Td \times 1$ vector obtained by rearranging $\bX$, and $\mathbf{Z}$ is a block diagonal matrix with diagonal component $Z^{\T} =(\bx_0,\ldots,\bx_{T-1})^{\T}$.

$f_{\lambda_1}(A;\bX,\bY)$ does not have a closed form solution due to the $\ell_1$ term. However, it can be solved numerically with a Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). The FISTA algorithm is detailed in the Appendix.

With FISTA, matrix $A$ can be updated as follows:
\begin{equation*}\label{eq:updatea}
A^{\text{new}} = \text{FISTA}(\|\mathbf{Z}^{\T}\mathbf{a}^{\text{old}} -\mathbf{z}\|_2^2,\quad \lambda_1)
\end{equation*}

\subsection{Initialization}\label{sec:initial}
The $R$ matrix is initialized as the identity matrix, while $\mathbf{\pi}_0$  is initialized as the $\mathbf{0}$ vector. For $A$ and $C$, denote $\bY = \left[\mathbf{y_1},\cdots,\mathbf{y_T}\right]$, a $p\times T$ matrix, then the singular value decomposition (SVD) of $\bY$ is $\bY = \mathbf{UDV^{\T}} \approx \mathbf{U}_{p \times d} \mathbf{D}_{d \times d} \mathbf{V}_{d \times T}^{\T} =\mathbf{U}_{p\times d}\bX_{d \times T}$, where $\mathbf{U}_{p \times d}$ is the first $d$ columns of $\mathbf{U}$ and $\mathbf{D}_{d\times d}$ is the upper left block of $\mathbf{D}$. This notation also applies to $\mathbf{V}^{\T}_{d \times T}$.

$C$ is then initialized as $\mathbf{U}_{p\times d}$, while the columns of $\bX_{d \times T}$ are used as input for a vector autoregressive (VAR) model to estimate the initial value for $A$.

\subsection{Improving Computational Efficiency}
The major factors that affect the efficiency and scalability of the above EM algorithm involve the storage and computations of the covariance matrix $R$, which is a ${p \times p}$ matrix. The following computational techniques are utilized to make the code highly efficient and scalable.
For the covariance matrix $R$, with constraint 4 (i.e. the diagonal assumption), we employ a sparse matrix to represent $R$, and only the diagonal elements are directly calculated.
In the E-step, the term $K_t=V_t^{t-1}C^{\T}(CV_t^{t-1}C^{\T}+R)^{-1}$ involves the inverse of a large square $p \times p$ matrix, which might be intractable. The Woodbury Matrix Identity is employed to turn a high dimensional matrix inverse to a low dimensional one: $(CV_t^{t-1}C^{\T}+R)^{-1} = R^{-1} - R^{-1}C[(V_t^{t-1})^{-1} + C^{\T}R^{-1}C]^{-1}C^{\T}R^{-1}$.

Note that quantities like $R^{-1}$ and $C^{\T}R^{-1}C$ can be pre-computed and reused throughout the E step. With the above three techniques, the EM algorithm can scale to very high dimensions in terms of $p$, $d$, and $T$, without causing any computational issues.

\section*{Appendix 3: FISTA Algorithm}
\label{sec:appendix2}
In general, FISTA optimize a target function
\begin{equation}\label{eqn: fistatarget}
\min_{\substack{x\in \mathcal{X}}}\quad \mathbf{F(x;\lambda)} = \mathbf{g(x)}+ \mathbf{\lambda \|x\|_1}
\end{equation}
where $\mathbf{g}: R^n \rightarrow R $ is a continuously differentiable convex function and $\lambda > 0$ is the regularization parameter. A FISTA algorithm with constant step is detailed below\\

%\begin{center}
\begin{tabular}{l}
\hline
\textbf{Algorithm } FISTA$(\mathbf{g},\lambda)$.\\
\hline
 1. Input an initial guess $\mathbf{x_0}$ and Lipschitz constant $\mathbf{L}$ for $\mathbf{\nabla g}$, set $\mathbf{y_1} = \mathbf{x_0},t_1 = 1$\\
 2. Choose $\tau \in (0,1/\mathbf{L}]$; Set $k \leftarrow$ 0.\\
 3. \textbf{loop}\\
 4. \hspace{10mm}		Evaluate $\mathbf{\nabla g(y_k)}$\\
 5.	\hspace{10mm}	Compute $\mathbf{x_{1}}$= $\mathbf{S_{\tau\lambda}(y_k - \tau\nabla g(y_k))}$\\
 6.	\hspace{10mm}	Compute $t_{k+1} = \frac{1+\sqrt{1 + 4 t_k^2}}{2}$\\
 7.	\hspace{10mm}	$\mathbf{y_{k+1}} = \mathbf{x_k} + \left(\frac{t_k - 1}{t_{k+1}})\right (\mathbf{x_k}-\mathbf{x_{k-1}})$\\
 8.	\hspace{10mm}	Set $k \leftarrow k+1$ \\
 9. \textbf{end loop}\\
\hline
\end{tabular}
%\end{center}


\vspace*{10mm}
In the above
\[
\mathbf{S_\lambda (y) = (|y|-\lambda)_{+}\textbf{sign}(y)}=\left\{
\begin{array}{l l }
 y - \lambda & \text{if   } y > \lambda\\
 y + \lambda & \text{if   } y < -\lambda\\
 0 & \text{if   } |y| \leq \lambda .
\end{array}
\right.
\]

The Lipschitz constant $L$ for $\nabla\mathbf{g(z)}=\mathbf{Z}^{\T}(\mathbf{Z}\mathbf{a} -\mathbf{z})$, where $\mathbf{g}(\mathbf{z})=\|\mathbf{Z}^{\T}\mathbf{a} -\mathbf{z}\|_2^2$, is calculated as follows. Denote $\|Z\|$ as the induced norm of matrix $Z$, then $L$ is
\[
L = \sup_{\substack{x\neq y}}\frac{\|\mathbf{Z}^{\T}(\mathbf{Z}x- \mathbf{Z}y)\|}{\|x-y\|}=\sup_{\substack{x\neq 0}}\frac{\|\mathbf{Z}^{\T}\mathbf{Z}x\|}{\|x\|}\leq\|\mathbf{Z}^{\T}\|\|\mathbf{Z}\| = \|Z^{\T}\|\|Z\|.
\]


\section*{Appendix 4: $k$-step predictions with PCA and \mrsid}
\label{sec:appendix3}
\begin{tabular}{l}
\hline
\textbf{Algorithm } $k$-step predictions with PCA and \mrsid\\
\hline
 1. Denote estimations with PCA and \mrsid~as $A_{pca},C_{pca},A_{plds},$ and $C_{plds}$ respectively.\\
 2. PCA estimated latent states at $t=1000$: $x_{1000,pca} = $ column 1000 of $\bX_{d\times T}$ from Section 3.3 \\
 3. \mrsid~estimated latent states at $t=1000$: $x_{1000,pls}$ is from the E step in Section 3.4\\
 4. \textbf{for i = 1 to k}\\
 5. \hspace{10mm}		$x_{1000+k,pca}\ =\ A_{pca}\ x_{999+k,pca}$\\
 6.	\hspace{10mm}	$y_{1000+k,pca}\ =\ C_{pca}\ x_{1000+k,pca}$\\
 7.	\hspace{10mm}	$x_{1000+k,plds}\ =\ A_{plds}\ x_{999+k,plds}$\\
 8.	\hspace{10mm}	$y_{1000+k,plds}\ =\ C_{plds}\ x_{1000+k,plds}$\\
 9. \textbf{end}\\
\hline
\end{tabular}

\section*{Appendix 5: Simulation Data Generation}
\label{sec:appendix4}
\begin{tabular}{l}
\hline
\textbf{Algorithm } Simulation Data Generation\\
\hline
 1. Denote the dimensions as $p$, $d$ and $T$ respectively\\
 2. Generate a $p\times d$ matrix $C_0$ from a standard Gaussian distribution\\
 3. Sort each column of $C_0$ in ascending order to get matrix $C$ \\
 4. Generate a $d\times d$ matrix $A_0$ from a standard Gaussian distribution\\
 5. Add a multiple of the identity matrix to $A_0$\\
 6.	Replace entries in $A_0$ with small absolute values with $0$\\
 7.	Scale $A_0$ to make sure its eigen values are between $-1$ and $1$; use $A_0$ as the A matrix\\
 8.	Let $R$ be a diagonal matrix with positive diagonal entries and $Q$ be the identity matrix \\
 9. Generate simulation data with $A, C, Q$ and $R$ \\
 10. \textbf{end}\\
\hline
\end{tabular}

\end{document}
